{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e289e5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import HTMLHeaderTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "15b3c8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../san_francisco-ca-2.html', encoding='latin-1') as f:\n",
    "    planning_code = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f4fe423d-f5c8-480b-954f-7b252c3cfd89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36203998"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(planning_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6bbd1a53-1029-420c-9177-790ca9e19107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_css_from_html(html_content):\n",
    "    # Credit to gpt4\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Remove all style attributes\n",
    "    for tag in soup.recursiveChildGenerator():\n",
    "        if hasattr(tag, 'attrs'):\n",
    "            tag.attrs = {key: value for key, value in tag.attrs.items() if key != 'style'}\n",
    "\n",
    "    # Remove all class attributes\n",
    "    for tag in soup.find_all(True, {'class': True}):\n",
    "        del tag['class']\n",
    "\n",
    "    # Find and remove all AnnotationDrawer elements\n",
    "    for tag in soup.find_all('annotationdrawer'):\n",
    "        tag.decompose()\n",
    "\n",
    "    # Remove all id and class attributes\n",
    "    for tag in soup.find_all(True):\n",
    "        tag.attrs = {key: value for key, value in tag.attrs.items() if key not in ['id', 'class']}\n",
    "\n",
    "    # Remove empty div tags\n",
    "    for div in soup.find_all('div', recursive=True):\n",
    "        if not div.text.strip() and not div.contents:\n",
    "            div.decompose()\n",
    "\n",
    "    # Remove custom codeoptions tags and span tags with depth attribute\n",
    "    for tag in soup.find_all(['codeoptions', 'span']):\n",
    "        if tag.name == 'span' and 'depth' in tag.attrs:\n",
    "            tag.decompose()\n",
    "        elif tag.name == 'codeoptions':\n",
    "            tag.decompose()\n",
    "            \n",
    "    for img_tag in soup.find_all('img'):\n",
    "        img_tag.decompose()\n",
    "        \n",
    "    return str(soup)\n",
    "\n",
    "# Remove CSS from the HTML\n",
    "clean_html = remove_css_from_html(planning_code)\n",
    "\n",
    "# Write the cleaned HTML to a new file\n",
    "with open('cleaned_html_file.html', 'w') as file:\n",
    "    file.write(clean_html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "73cca756-23d4-447c-ac78-646ac366c76e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38264265731094116"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_html) / len(planning_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "37b13823-fa1c-4972-b183-36462e2c8892",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = HTMLHeaderTextSplitter(headers_to_split_on=['div.ChapAn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e05b87b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter2 = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 50,\n",
    "    length_function = len,\n",
    "    add_start_index = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff15ede",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chunks = splitter.split_text(clean_html)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fff332-52e5-432e-8744-efcc88cade2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 500\n",
    "chunk_overlap = 30\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "420882f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_key = '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'\n",
    "import openai\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings_model = OpenAIEmbeddings(api_key=openai_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5cf9b88",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or buffer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/alex/housing/housing/Untitled.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alex/housing/housing/Untitled.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m chunk_embeddings \u001b[39m=\u001b[39m embeddings_model\u001b[39m.\u001b[39;49membed_documents(chunks)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/housing-s88OZGFv-py3.11/lib/python3.11/site-packages/langchain/embeddings/openai.py:556\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[0;34m(self, texts, chunk_size)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[39m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[1;32m    554\u001b[0m \u001b[39m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[1;32m    555\u001b[0m engine \u001b[39m=\u001b[39m cast(\u001b[39mstr\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeployment)\n\u001b[0;32m--> 556\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_len_safe_embeddings(texts, engine\u001b[39m=\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/housing-s88OZGFv-py3.11/lib/python3.11/site-packages/langchain/embeddings/openai.py:409\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m001\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    406\u001b[0m     \u001b[39m# See: https://github.com/openai/openai-python/issues/418#issuecomment-1525939500\u001b[39;00m\n\u001b[1;32m    407\u001b[0m     \u001b[39m# replace newlines, which can negatively affect performance.\u001b[39;00m\n\u001b[1;32m    408\u001b[0m     text \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 409\u001b[0m token \u001b[39m=\u001b[39m encoding\u001b[39m.\u001b[39;49mencode(\n\u001b[1;32m    410\u001b[0m     text,\n\u001b[1;32m    411\u001b[0m     allowed_special\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mallowed_special,\n\u001b[1;32m    412\u001b[0m     disallowed_special\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdisallowed_special,\n\u001b[1;32m    413\u001b[0m )\n\u001b[1;32m    414\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(token), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_ctx_length):\n\u001b[1;32m    415\u001b[0m     tokens\u001b[39m.\u001b[39mappend(token[j : j \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_ctx_length])\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/housing-s88OZGFv-py3.11/lib/python3.11/site-packages/tiktoken/core.py:116\u001b[0m, in \u001b[0;36mEncoding.encode\u001b[0;34m(self, text, allowed_special, disallowed_special)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(disallowed_special, \u001b[39mfrozenset\u001b[39m):\n\u001b[1;32m    115\u001b[0m         disallowed_special \u001b[39m=\u001b[39m \u001b[39mfrozenset\u001b[39m(disallowed_special)\n\u001b[0;32m--> 116\u001b[0m     \u001b[39mif\u001b[39;00m match \u001b[39m:=\u001b[39m _special_token_regex(disallowed_special)\u001b[39m.\u001b[39;49msearch(text):\n\u001b[1;32m    117\u001b[0m         raise_disallowed_special_token(match\u001b[39m.\u001b[39mgroup())\n\u001b[1;32m    119\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or buffer"
     ]
    }
   ],
   "source": [
    "chunk_embeddings = embeddings_model.embed_documents([c['page_content'] for c in chunks])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
